{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11799d81",
   "metadata": {},
   "source": [
    "# Llama‑2‑7B‑Chat QLoRA SFT — Customer Service (English)\n",
    "\n",
    "이 notebook은 meta-llama/Llama-2-7b-chat-hf 모델을 고객 지원(customer-support) 데이터셋으로 파인튜닝하여,\n",
    "“환불은 어떻게 받을 수 있나요?” 같은 질문에 명확하고 단계적인 영어 답변을 제공하도록 학습합니다.\n",
    "\n",
    "**Datasets (Hugging Face):**\n",
    "- `argilla/customer_assistant`\n",
    "- `argilla/synthetic-sft-customer-support-single-turn`\n",
    "- `bitext/Bitext-customer-support-llm-chatbot-training-dataset`\n",
    "\n",
    "**Training recipe:** TRL의 SFTTrainer와 **PEFT QLoRA(4비트)**를 사용하며,\n",
    "채팅 형식의 프롬프트 템플릿으로 학습하되 assistant의 응답에만 손실(loss) 을 계산합니다.\n",
    "마지막에는 빠른 추론(inference) 예시가 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5f14a7-a5d3-4205-a0e7-321d8cec748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install --upgrade pip\n",
    "%pip -q install \\\n",
    "  \"transformers>=4.43,<4.47\" \\\n",
    "  \"accelerate>=0.33\" \\\n",
    "  \"datasets>=2.20\" \\\n",
    "  \"trl==0.9.6\" \\\n",
    "  \"peft>=0.12\" \\\n",
    "  \"einops\" \\\n",
    "  \"sentencepiece\" \\\n",
    "  \"hf_transfer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345aa4f4-7d92-47e5-b2c5-002ffd61aecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ bitsandbytes installed.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, subprocess\n",
    "\n",
    "if platform.system() == \"Linux\":\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bitsandbytes>=0.43\"])\n",
    "        print(\"bitsandbytes installed.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"bitsandbytes install failed. Check CUDA/GPU runtime.\")\n",
    "else:\n",
    "    print(\"Non-Linux detected — skipping bitsandbytes. (Use full precision or provide a compatible wheel.)\")\n",
    "\n",
    "# (Optional) avoid Accelerate interactive prompt later\n",
    "try:\n",
    "    from accelerate.utils import write_basic_config\n",
    "    write_basic_config()\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b6cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, json, re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "                          TrainingArguments, AutoConfig)\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9146f",
   "metadata": {},
   "source": [
    "## Login to Hugging Face (required for Llama‑2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0799259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF whoami: {'type': 'user', 'id': '6900911d2cafc5f572673a1c', 'name': 'survd0404', 'fullname': 'Lee', 'isPro': False, 'avatarUrl': '/avatars/6738b5fc2f71a66ab3ca028ab9a5da26.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'HF_TOKEN', 'role': 'fineGrained', 'createdAt': '2025-11-05T14:30:18.423Z', 'fineGrained': {'canReadGatedRepos': True, 'global': ['discussion.write', 'post.write'], 'scoped': [{'entity': {'_id': '6900911d2cafc5f572673a1c', 'type': 'user', 'name': 'survd0404'}, 'permissions': ['repo.content.read', 'repo.write', 'inference.serverless.write', 'inference.endpoints.infer.write', 'inference.endpoints.write', 'user.webhooks.read', 'user.webhooks.write', 'collection.read', 'collection.write', 'discussion.write']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "# --- Hugging Face login helper ---\n",
    "try:\n",
    "    from huggingface_hub import login, whoami\n",
    "    import os\n",
    "    if os.environ.get(\"HF_TOKEN\"):\n",
    "        login(token=os.environ[\"HF_TOKEN\"])\n",
    "        try:\n",
    "            print(\"HF whoami:\", whoami())\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        print(\" Set HF_TOKEN env var or run:\")\n",
    "        print(\"   from huggingface_hub import login; login(token='hf_...')\")\n",
    "except Exception as e:\n",
    "    print(\"Hugging Face login not available:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f2cfa",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72949619",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "DATASETS = [\n",
    "    (\"argilla/customer_assistant\", {}),\n",
    "    (\"argilla/synthetic-sft-customer-support-single-turn\", {}),\n",
    "    (\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\", {}),\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"out_llama2_cs_qlora\"\n",
    "SEED = 42\n",
    "\n",
    "# QLoRA / training settings tuned for a 24GB GPU; reduce if you get OOM.\n",
    "MAX_SEQ_LEN = 1024\n",
    "PER_DEVICE_TRAIN_BS = 1\n",
    "GRAD_ACCUM = 8\n",
    "EPOCHS = 2\n",
    "LR = 2e-4\n",
    "WARMUP_RATIO = 0.03\n",
    "\n",
    "# LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Response template for loss masking\n",
    "RESPONSE_TEMPLATE = \"\\n### Assistant:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5e44a",
   "metadata": {},
   "source": [
    "## Load tokenizer & quantized base model (4‑bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "758b0dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de1f55a3ad640e49cc12bd36ff466d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630c2d47cf9b4439b8805a9722016cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc522ea37974a11b2c24a9821452e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e164c3e18b40fba996dab6c0d636a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ba9bd2146141b8bad55f819b6a3c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d11762b3e14aa489079465e4b0809b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,976,960 || all params: 6,778,392,576 || trainable%: 0.5898\n"
     ]
    }
   ],
   "source": [
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=os.getenv(\"HF_TOKEN\"))\n",
    "# Ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_cfg,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    ")\n",
    "\n",
    "# Attach LoRA\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96bbbe",
   "metadata": {},
   "source": [
    "## Dataset normalizer\n",
    "We’ll coerce each dataset into a common schema: `{user, assistant}` with optional `system`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6857646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: argilla/customer_assistant {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad2f19c09a14417a30e29cdce4f3561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b1ed7f4ef24ac5a2dbbd452541c084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-6853b30b1d9b88(…):   0%|          | 0.00/76.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81409d836ed3463cbdc748c25a3e492f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebedd03b57294f4b8b257cc4410abe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9e27aa35124afeaf3b6bc35978b010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: argilla/synthetic-sft-customer-support-single-turn {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153141963aec43849447c6ff339e43c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2583225800fe4bd3b56b62f76c13786b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/133k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01556911900949da8a6145924f907867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb3b37aadee4a218badbb257084ed62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54684071ba7c4975962ebd358aa96c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: bitext/Bitext-customer-support-llm-chatbot-training-dataset {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a35c681ba94599a42ec686eec15b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09891cfb40a48a8ac0def1190a70796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Bitext_Sample_Customer_Support_Training_(…):   0%|          | 0.00/19.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003006707dd3419aa984815caa65b326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/26872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc233ab9d4264caf83216d2c854a9b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508c7ce988c54b3e903503d91536012d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "### System:\n",
      "You are a helpful, step-by-step customer support assistant.\n",
      "### User:\n",
      "I want assistance retrieving my user profile key\n",
      "### Assistant:\n",
      "Thanks for dropping us a line to us for assistance with retrieving your user profile key. I understand how important it is for you to access your account and retrieve the key. To assist you, please follow these steps:\n",
      "\n",
      "1. Visit the login page of our plat\n",
      "----\n",
      "### System:\n",
      "You are a helpful, step-by-step customer support assistant.\n",
      "### User:\n",
      "where to inform of problems with online payments ?\n",
      "### Assistant:\n",
      "To inform us about any problems you encounter with online payments, you can reach out to our dedicated customer support team. They are available {{Customer Support Hours}} at {{Customer Support Phone Number}}, or you can chat with them through the Live\n",
      "\n",
      "Total examples: 27168\n"
     ]
    }
   ],
   "source": [
    "def extract_user_assistant(record: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Heuristic field mapping across the three datasets.\n",
    "    Returns keys: user, assistant, and optional system.\n",
    "    \"\"\"\n",
    "    keys = set(record.keys())\n",
    "\n",
    "    # Common patterns\n",
    "    user_candidates = [\n",
    "        \"user\", \"question\", \"prompt\", \"instruction\", \"input\", \"request\", \"query\", \"messages_user\", \"customer\", \"customer_message\"\n",
    "    ]\n",
    "    assistant_candidates = [\n",
    "        \"assistant\", \"answer\", \"response\", \"output\", \"messages_assistant\", \"agent\", \"agent_response\"\n",
    "    ]\n",
    "    system_candidates = [\"system\", \"context\", \"role\", \"scenario\"]\n",
    "\n",
    "    def pick(cands):\n",
    "        for c in cands:\n",
    "            if c in record and isinstance(record[c], str) and len(record[c].strip()) > 0:\n",
    "                return record[c]\n",
    "        # Look for nested message lists (e.g., [{\"role\":\"user\",\"content\":...}, ...])\n",
    "        for k in keys:\n",
    "            v = record[k]\n",
    "            if isinstance(v, list) and len(v)>0 and isinstance(v[0], dict) and \"role\" in v[0] and \"content\" in v[0]:\n",
    "                # try to assemble conversation\n",
    "                user_msg = None\n",
    "                assistant_msg = None\n",
    "                system_msg = None\n",
    "                for m in v:\n",
    "                    role = m.get(\"role\",\"\")\n",
    "                    content = m.get(\"content\",\"\")\n",
    "                    if role == \"system\" and not system_msg:\n",
    "                        system_msg = content\n",
    "                    if role in (\"user\",\"customer\") and not user_msg:\n",
    "                        user_msg = content\n",
    "                    if role in (\"assistant\",\"agent\") and not assistant_msg:\n",
    "                        assistant_msg = content\n",
    "                if cands is user_candidates and user_msg:\n",
    "                    return user_msg\n",
    "                if cands is assistant_candidates and assistant_msg:\n",
    "                    return assistant_msg\n",
    "                if cands is system_candidates and system_msg:\n",
    "                    return system_msg\n",
    "        return None\n",
    "\n",
    "    user = pick(user_candidates)\n",
    "    assistant = pick(assistant_candidates)\n",
    "    system = pick(system_candidates) or \"You are a helpful, step-by-step customer support assistant.\"\n",
    "\n",
    "    if not user or not assistant:\n",
    "        # Fallback: try to synthesize from any two text-like fields\n",
    "        textish = [k for k in keys if isinstance(record[k], str) and len(record[k].strip())>0]\n",
    "        if len(textish) >= 2 and not user:\n",
    "            user = record[textish[0]]\n",
    "        if len(textish) >= 2 and not assistant:\n",
    "            assistant = record[textish[1]]\n",
    "\n",
    "    if not user or not assistant:\n",
    "        raise ValueError(\"Could not map record to user/assistant: keys=%s\" % list(keys))\n",
    "\n",
    "    return {\"user\": user.strip(), \"assistant\": assistant.strip(), \"system\": system.strip()}\n",
    "\n",
    "def make_text(example: Dict[str, str]) -> str:\n",
    "    # Simple, stable chat template compatible with response-only loss masking.\n",
    "    # We place a clear response boundary: `\\n### Assistant:\\n`\n",
    "    sys_ = example.get(\"system\",\"You are a helpful, step-by-step customer support assistant.\")\n",
    "    usr = example[\"user\"]\n",
    "    asst = example[\"assistant\"]\n",
    "    return f\"\"\"### System:\n",
    "{sys_}\n",
    "### User:\n",
    "{usr}\n",
    "### Assistant:\n",
    "{asst}\"\"\"\n",
    "\n",
    "def preprocess_dataset(hf_id: str, hf_config: Dict[str, Any]):\n",
    "    ds = load_dataset(hf_id, **hf_config)\n",
    "    split_names = [k for k in ds.keys()] if isinstance(ds, DatasetDict) else [\"train\"]\n",
    "    out_splits = []\n",
    "    for split in split_names:\n",
    "        d = ds[split] if isinstance(ds, DatasetDict) else ds\n",
    "        # Filter/map to unified schema\n",
    "        def _map_fn(rec):\n",
    "            try:\n",
    "                ex = extract_user_assistant(rec)\n",
    "                return {\"text\": make_text(ex)}\n",
    "            except Exception:\n",
    "                return {\"text\": None}\n",
    "        d2 = d.map(_map_fn, remove_columns=[c for c in d.column_names if c!=\"text\"])\n",
    "        d2 = d2.filter(lambda r: r[\"text\"] is not None and len(r[\"text\"])>20)\n",
    "        out_splits.append(d2)\n",
    "    # Concatenate all available splits\n",
    "    merged = concatenate_datasets(out_splits) if len(out_splits)>1 else out_splits[0]\n",
    "    return merged\n",
    "\n",
    "all_ds = []\n",
    "for name, cfg in DATASETS:\n",
    "    print(\"Loading:\", name, cfg)\n",
    "    try:\n",
    "        all_ds.append(preprocess_dataset(name, cfg))\n",
    "    except Exception as e:\n",
    "        print(\"!! Skipped\", name, \"due to\", e)\n",
    "\n",
    "assert len(all_ds) > 0, \"No datasets loaded—check access or names.\"\n",
    "merged = concatenate_datasets(all_ds)\n",
    "merged = merged.shuffle(seed=SEED)\n",
    "\n",
    "# Quick peek\n",
    "for i in range(2):\n",
    "    print(\"----\")\n",
    "    print(merged[i][\"text\"][:400])\n",
    "print(\"\\nTotal examples:\", len(merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0386025",
   "metadata": {},
   "source": [
    "## Tokenization + Loss Masking (Assistant-only)\n",
    "We’ll mask loss to the tokens after the `RESPONSE_TEMPLATE` boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ece54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build response template ids\n",
    "response_template_ids = tokenizer.encode(RESPONSE_TEMPLATE, add_special_tokens=False)\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=RESPONSE_TEMPLATE,\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# No need for a separate tokenization function; SFTTrainer will handle packing & truncation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083258b",
   "metadata": {},
   "source": [
    "## Train with TRL SFTTrainer (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10411143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter saved to: out_llama2_cs_qlora/lora_adapter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e594cd63854ee38d4ccb5da0fd3e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to: out_llama2_cs_qlora/merged\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "ADAPTER_DIR = os.path.join(OUTPUT_DIR, \"lora_adapter\")\n",
    "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
    "\n",
    "# Save LoRA adapter (creates adapter_config.json + adapter_model.safetensors)\n",
    "trainer.model.save_pretrained(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Adapter saved to:\", ADAPTER_DIR)\n",
    "\n",
    "# === merge LoRA into base (optional; needs VRAM) ===\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "\n",
    "    def find_adapter_dir(base_dir: str) -> str:\n",
    "        cand = os.path.join(base_dir, \"lora_adapter\")\n",
    "        if os.path.exists(os.path.join(cand, \"adapter_config.json\")):\n",
    "            return cand\n",
    "        ckpts = sorted(\n",
    "            glob.glob(os.path.join(base_dir, \"checkpoint-*\")),\n",
    "            key=lambda p: int(p.split(\"-\")[-1]),\n",
    "            reverse=True\n",
    "        )\n",
    "        for c in ckpts:\n",
    "            for sub in (\"\", \"lora_adapter\"):\n",
    "                d = os.path.join(c, sub)\n",
    "                if os.path.exists(os.path.join(d, \"adapter_config.json\")):\n",
    "                    return d\n",
    "        raise FileNotFoundError(f\"No adapter_config.json found under {base_dir}\")\n",
    "\n",
    "    adapter_dir = find_adapter_dir(OUTPUT_DIR)\n",
    "\n",
    "    # Load base in full precision for merge\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=None,\n",
    "        torch_dtype=torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        token=os.getenv(\"HF_TOKEN\"),\n",
    "    )\n",
    "\n",
    "    peft_model = PeftModel.from_pretrained(base, adapter_dir)\n",
    "    merged_model = peft_model.merge_and_unload()  # -> plain HF model with LoRA merged\n",
    "\n",
    "    merged_dir = os.path.join(OUTPUT_DIR, \"merged\")\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    merged_model.save_pretrained(merged_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "    print(\"Merged model saved to:\", merged_dir)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Merge skipped due to:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551aff1",
   "metadata": {},
   "source": [
    "## (Optional) Merge LoRA into Base Weights and Save\n",
    "Merging requires enough VRAM; you can skip and use the adapter in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83dfdddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ebb9326a5340acbb64a00ea9422ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to: out_llama2_cs_qlora/merged\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from peft import PeftModel\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=None,  # load in full precision for merge\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        token=os.getenv(\"HF_TOKEN\"),\n",
    "    )\n",
    "    peft_model = PeftModel.from_pretrained(\n",
    "        base,\n",
    "        os.path.join(OUTPUT_DIR, \"lora_adapter\"),\n",
    "    )\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    merged_dir = os.path.join(OUTPUT_DIR, \"merged\")\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    merged_model.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "    print(\"Merged model saved to:\", merged_dir)\n",
    "except Exception as e:\n",
    "    print(\"Merge skipped due to:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ca4cf",
   "metadata": {},
   "source": [
    "## Inference demo: “How can I get a refund?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3c009f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82078856e9c484b8886f18e1cc925fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, thank you for reaching out to us! To initiate a refund, please follow these steps:\n",
      "\n",
      "1. Check your order history on our website to ensure that the item you want a refund for is eligible for a return.\n",
      "2. Contact our customer service team via email or phone to request a return. Please include your order number and the reason for the return.\n",
      "3. Once we receive your return request, we will provide you with a return shipping label and instructions on how to return the item.\n",
      "4. Once we receive the returned item, we will process your refund within 3-5 business days.\n",
      "\n",
      "Please let me know if you have any other questions or concerns!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load with adapter for inference (safe VRAM)\n",
    "inf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_cfg,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    ")\n",
    "from peft import PeftModel\n",
    "inf_model = PeftModel.from_pretrained(inf_model, os.path.join(OUTPUT_DIR, \"lora_adapter\"))\n",
    "inf_model.eval()\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=inf_model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "def build_prompt(user_question: str, system: str = \"You are a helpful, step-by-step customer support assistant.\"):\n",
    "    return f\"\"\"### System:\n",
    "{system}\n",
    "### User:\n",
    "{user_question}\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "prompt = build_prompt(\"How can I get a refund?\")\n",
    "out = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.9, eos_token_id=tokenizer.eos_token_id)\n",
    "print(out[0][\"generated_text\"][len(prompt):].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
