{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ad1f38",
   "metadata": {},
   "source": [
    "\n",
    "# SFT with Template Masking (LoRA) â€” Argilla/Bitext + Dolly (7B)\n",
    "\n",
    "This notebook fine-tunes a chat model (e.g., `meta-llama/Llama-2-7b-hf`) using **template masking** (no collator), so only the **assistant** parts are trained.  \n",
    "Key points:\n",
    "\n",
    "- **No DataCollator** required. We use the tokenizer's **chat template** with `{% generation %}...{% endgeneration %}` to mask assistant turns.\n",
    "- `packing=True` works (higher throughput) since masking is handled by the chat template.\n",
    "- `tokenizer.truncation_side=\"left\"` to preserve the assistant turn at the end of long sequences.\n",
    "- Supports Argilla/Bitext customer-support datasets + Dolly-15k subset.\n",
    "- Uses **LoRA** (PEFT) to reduce VRAM usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9275a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%capture\n",
    "# # (Optional) Install versions known to work well together\n",
    "!pip install -U transformers accelerate datasets peft bitsandbytes\n",
    "!pip install -U trl>=0.25.0\n",
    "# # (Optional) flash-attn (environment dependent)\n",
    "# !pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711dad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, Any, List\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596eecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83625cc4c2a648babba0e31dbacb9464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose your base model (7B chat model recommended)\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"meta-llama/Llama-2-7b-hf\")\n",
    "# You must have access to the model on HF. If needed: `huggingface-cli login`\n",
    "\n",
    "print(\"Loading tokenizer:\", MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# Truncation/padding so that assistant turn (at the end) is kept\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.padding_side    = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model:\", MODEL_NAME)\n",
    "# Use bf16 if on multi-GPU auto mapping; fp16 if single 24GB GPU for lower VRAM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",     # change to torch.bfloat16 / torch.float16 explicitly if desired\n",
    "    device_map=\"auto\"       # let HF place layers automatically if multiple GPUs\n",
    ")\n",
    "# Disable cache during training\n",
    "if hasattr(model, \"config\"):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "print(\"Model & tokenizer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad23501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat template with assistant masking is set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a chat template that masks assistant turns only.\n",
    "# With this, SFTTrainer will compute loss only on `{% generation %}` blocks.\n",
    "tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "<|{{ message['role'] }}|>\n",
    "{% if message['role'] == 'assistant' -%}\n",
    "{% generation %}\n",
    "{{ message['content'] }}\n",
    "{% endgeneration %}\n",
    "{%- else -%}\n",
    "{{ message['content'] }}\n",
    "{%- endif %}\n",
    "\n",
    "{% endfor %}\"\"\"\n",
    "\n",
    "print(\"Chat template with assistant masking is set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4450b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Datasets to pull\n",
    "ARGILLA_BITEXT = [\n",
    "    (\"argilla/customer_assistant\", {}),\n",
    "    (\"argilla/synthetic-sft-customer-support-single-turn\", {}),\n",
    "    (\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\", {}),\n",
    "]\n",
    "DOLLY_SLICE = \"train[:1000]\"  # Adjust as needed\n",
    "\n",
    "def extract_user_assistant(record: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Map heterogeneous keys to a unified (system, user, assistant) triple.\"\"\"\n",
    "    keys = set(record.keys())\n",
    "\n",
    "    user_candidates = [\n",
    "        \"user\",\"question\",\"prompt\",\"instruction\",\"input\",\"request\",\"query\",\n",
    "        \"messages_user\",\"customer\",\"customer_message\"\n",
    "    ]\n",
    "    assistant_candidates = [\n",
    "        \"assistant\",\"answer\",\"response\",\"output\",\"messages_assistant\",\n",
    "        \"agent\",\"agent_response\"\n",
    "    ]\n",
    "    system_candidates = [\"system\",\"context\",\"role\",\"scenario\"]\n",
    "\n",
    "    def pick(cands):\n",
    "        # 1) flat keys\n",
    "        for c in cands:\n",
    "            if c in record and isinstance(record[c], str) and len(record[c].strip())>0:\n",
    "                return record[c]\n",
    "        # 2) nested messages\n",
    "        for k in keys:\n",
    "            v = record[k]\n",
    "            if isinstance(v, list) and len(v)>0 and isinstance(v[0], dict) and \"role\" in v[0] and \"content\" in v[0]:\n",
    "                u, a, s = None, None, None\n",
    "                for m in v:\n",
    "                    role = (m.get(\"role\",\"\") or \"\").lower()\n",
    "                    content = m.get(\"content\",\"\")\n",
    "                    if role == \"system\" and not s:\n",
    "                        s = content\n",
    "                    if role in (\"user\",\"customer\") and not u:\n",
    "                        u = content\n",
    "                    if role in (\"assistant\",\"agent\") and not a:\n",
    "                        a = content\n",
    "                if cands is user_candidates and u:\n",
    "                    return u\n",
    "                if cands is assistant_candidates and a:\n",
    "                    return a\n",
    "                if cands is system_candidates and s:\n",
    "                    return s\n",
    "        return None\n",
    "\n",
    "    user = pick(user_candidates)\n",
    "    assistant = pick(assistant_candidates)\n",
    "    system = pick(system_candidates) or \"You are a helpful, step-by-step customer support assistant.\"\n",
    "\n",
    "    # fallback: synthesize from any two text fields\n",
    "    if not user or not assistant:\n",
    "        textish = [k for k in keys if isinstance(record.get(k), str) and len(record[k].strip())>0]\n",
    "        if len(textish)>=2 and not user:\n",
    "            user = record[textish[0]]\n",
    "        if len(textish)>=2 and not assistant:\n",
    "            assistant = record[textish[1]]\n",
    "\n",
    "    if not user or not assistant:\n",
    "        raise ValueError(\"Could not map record to user/assistant\")\n",
    "\n",
    "    return {\"system\": system.strip(), \"user\": user.strip(), \"assistant\": assistant.strip()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0389b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_to_messages(hf_id: str, hf_config: Dict[str, Any]):\n",
    "    ds = load_dataset(hf_id, **hf_config)\n",
    "    split_names = list(ds.keys()) if isinstance(ds, DatasetDict) else [\"train\"]\n",
    "    out_splits = []\n",
    "\n",
    "    for split in split_names:\n",
    "        d = ds[split] if isinstance(ds, DatasetDict) else ds\n",
    "        def _map(rec):\n",
    "            try:\n",
    "                ex = extract_user_assistant(rec)\n",
    "                msgs = []\n",
    "                if ex.get(\"system\"):\n",
    "                    msgs.append({\"role\":\"system\",\"content\":ex[\"system\"]})\n",
    "                msgs.append({\"role\":\"user\",\"content\":ex[\"user\"]})\n",
    "                msgs.append({\"role\":\"assistant\",\"content\":ex[\"assistant\"]})\n",
    "                return {\"messages\": msgs}\n",
    "            except Exception:\n",
    "                return {\"messages\": None}\n",
    "\n",
    "        d2 = d.map(_map, remove_columns=[c for c in d.column_names if c!=\"messages\"])\n",
    "        d2 = d2.filter(lambda r: r[\"messages\"] is not None)\n",
    "        out_splits.append(d2)\n",
    "\n",
    "    merged = concatenate_datasets(out_splits) if len(out_splits)>1 else out_splits[0]\n",
    "    return merged\n",
    "\n",
    "def build_argilla_bitext_messages():\n",
    "    all_ds = []\n",
    "    for name, cfg in ARGILLA_BITEXT:\n",
    "        print(\"Loading:\", name, cfg)\n",
    "        try:\n",
    "            all_ds.append(preprocess_to_messages(name, cfg))\n",
    "        except Exception as e:\n",
    "            print(\"!! Skipped\", name, \"due to\", e)\n",
    "    if not all_ds:\n",
    "        raise RuntimeError(\"No argilla/bitext datasets loaded â€” check access/names.\")\n",
    "    merged = concatenate_datasets(all_ds)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da48967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_dolly_messages(slice_spec=DOLLY_SLICE):\n",
    "    ds = load_dataset(\"databricks/databricks-dolly-15k\", split=slice_spec)\n",
    "    def _fmt(sample):\n",
    "        instruction = sample.get(\"instruction\",\"\") or \"\"\n",
    "        context = sample.get(\"context\",\"\") or \"\"\n",
    "        response = sample.get(\"response\",\"\") or \"\"\n",
    "        user = f\"{instruction.strip()} {context.strip()}\".strip()\n",
    "        assistant = response.strip()\n",
    "        msgs = []\n",
    "        # Dolly usually has no system by default\n",
    "        if user:\n",
    "            msgs.append({\"role\":\"user\",\"content\":user})\n",
    "        if assistant:\n",
    "            msgs.append({\"role\":\"assistant\",\"content\":assistant})\n",
    "        return {\"messages\": msgs if msgs else None}\n",
    "\n",
    "    ds = ds.map(_fmt, remove_columns=[c for c in ds.column_names if c!=\"messages\"])\n",
    "    ds = ds.filter(lambda r: r[\"messages\"] is not None)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3366ce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: argilla/customer_assistant {}\n",
      "Loading: argilla/synthetic-sft-customer-support-single-turn {}\n",
      "Loading: bitext/Bitext-customer-support-llm-chatbot-training-dataset {}\n",
      "Merged size (raw): 28168\n",
      "After basic filtering: 28168\n",
      "---- <|system|>\n",
      "You are a helpful, step-by-step customer support assistant.\n",
      "<|user|>\n",
      "I want help seeing in which cases can I ask for a refund\n",
      "<|assistant|>\n",
      "Of course, I'm here to provide you with a detailed understanding of the cases in which you can request a refund. Here are some common scenarios:\n",
      "\n",
      "1. **Product/Service Defect:** If the product or service you purchased is defective, damaged, or doesn'\n",
      "---- <|system|>\n",
      "You are a helpful, step-by-step customer support assistant.\n",
      "<|user|>\n",
      "i have a problem setting a delivery address up\n",
      "<|assistant|>\n",
      "I apologize for the difficulties you're experiencing while setting up your delivery address. I'm here to assist you in resolving this issue.\n",
      "\n",
      "Setting up a delivery address is an essential part of the process, and I completely understand the importance of gett\n"
     ]
    }
   ],
   "source": [
    "\n",
    "argilla_bitext = build_argilla_bitext_messages()\n",
    "dolly_subset   = build_dolly_messages(DOLLY_SLICE)\n",
    "\n",
    "dataset = concatenate_datasets([argilla_bitext, dolly_subset]).shuffle(seed=SEED)\n",
    "print(\"Merged size (raw):\", len(dataset))\n",
    "\n",
    "def has_assistant(ex):\n",
    "    m = ex.get(\"messages\", [])\n",
    "    roles = [x.get(\"role\") for x in m]\n",
    "    return isinstance(m, list) and (\"assistant\" in roles)\n",
    "\n",
    "def min_tokens_chat(ex, min_tokens=8):\n",
    "    s = tokenizer.apply_chat_template(ex[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "    ids = tokenizer(s, add_special_tokens=False).input_ids\n",
    "    return ids is not None and len(ids) >= min_tokens\n",
    "\n",
    "dataset = dataset.filter(has_assistant)\n",
    "dataset = dataset.filter(min_tokens_chat)\n",
    "print(\"After basic filtering:\", len(dataset))\n",
    "\n",
    "# Quick peek\n",
    "for i in range(2):\n",
    "    s = tokenizer.apply_chat_template(dataset[i][\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "    print(\"----\", s[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6668c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 28068 eval: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = len(dataset)\n",
    "k = min(100, max(1, n // 50))  # about 2% or cap at 100\n",
    "split = dataset.train_test_split(test_size=k, seed=SEED, shuffle=True)\n",
    "train_dataset, eval_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(\"train:\", len(train_dataset), \"eval:\", len(eval_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12049e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA config ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],  # add k_proj/o_proj if needed\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "print(\"LoRA config ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44e410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTConfig(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "activation_offloading=False,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "assistant_only_loss=False,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=True,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "chat_template_path=None,\n",
      "completion_only_loss=None,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset_kwargs=None,\n",
      "dataset_num_proc=None,\n",
      "dataset_text_field=text,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eos_token=<EOS_TOKEN>,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_packing=None,\n",
      "eval_steps=100,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=no,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./sft_custom_results/runs/Nov12_08-31-30_88ba0954c455,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=20,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "loss_type=nll,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_length=512,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "model_init_kwargs=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./sft_custom_results,\n",
      "overwrite_output_dir=False,\n",
      "packing=True,\n",
      "packing_strategy=bfd,\n",
      "pad_to_multiple_of=None,\n",
      "pad_token=<PAD_TOKEN>,\n",
      "padding_free=False,\n",
      "parallelism_config=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "project=huggingface,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=200,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trackio_space_id=trackio,\n",
      "use_cpu=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=\"./sft_custom_results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=True,                 # On multi-GPU auto mapping, bf16=True can be safer\n",
    "    packing=True,              # âœ… works with template masking\n",
    "    max_length=512,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,   # big VRAM saver\n",
    ")\n",
    "print(sft_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fda7962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7edd466f8e4f77ae299adceb2a7cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/28068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdc22e61c194af9be4611c6fa947407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/28068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290422d4cdd14c05b83adcfce047f182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b839156dd34995ae6a0ebd700310e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready. Rendering one sample after template:\n",
      "<|system|>\n",
      "You are a helpful, step-by-step customer support assistant.\n",
      "<|user|>\n",
      "i have to see the status of purchase {{Order Number}} how do i do it\n",
      "<|assistant|>\n",
      "Glad you contacted us! I'm clearly cognizant that you would like to check the status of your purchase with order number {{Order Number}}. To do so, you can navigate to the 'Order Details' section on our website. This section will provide you with all the information regarding your purchase, including its current status. If you have any further questions or need additional assistance, please don't hesitate to ask. I'm here to help!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_args,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=None,\n",
    ")\n",
    "print(\"Trainer ready. Rendering one sample after template:\")\n",
    "print(tokenizer.apply_chat_template(train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False)[:600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17fcab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='667' max='667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [667/667 31:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.395400</td>\n",
       "      <td>1.356068</td>\n",
       "      <td>1.455166</td>\n",
       "      <td>798975.000000</td>\n",
       "      <td>0.661916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.000900</td>\n",
       "      <td>1.028595</td>\n",
       "      <td>1.016358</td>\n",
       "      <td>1598310.000000</td>\n",
       "      <td>0.729692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.927800</td>\n",
       "      <td>0.968086</td>\n",
       "      <td>0.963843</td>\n",
       "      <td>2397616.000000</td>\n",
       "      <td>0.740626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.880800</td>\n",
       "      <td>0.929540</td>\n",
       "      <td>0.924025</td>\n",
       "      <td>3196625.000000</td>\n",
       "      <td>0.746657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.897200</td>\n",
       "      <td>0.906796</td>\n",
       "      <td>0.909176</td>\n",
       "      <td>3996097.000000</td>\n",
       "      <td>0.751434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.895678</td>\n",
       "      <td>0.894779</td>\n",
       "      <td>4796144.000000</td>\n",
       "      <td>0.754299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "TrainOutput(global_step=667, training_loss=1.040491234237465, metrics={'train_runtime': 1886.3211, 'train_samples_per_second': 5.65, 'train_steps_per_second': 0.354, 'total_flos': 2.1139400014041907e+17, 'train_loss': 1.040491234237465, 'epoch': 1.0})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ðŸš€ Train\n",
    "train_result = trainer.train()\n",
    "print(\"Training complete.\")\n",
    "print(train_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "162d7c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|assistant|>\n",
      "I can see that you're eager to keep tabs on the progress of your order. To track your order, please provide me with your order number or the order reference number. With this information, I'll be able to provide you with the latest updates and provide any assistance you may need.\n",
      "\n",
      "<|assistant|>\n",
      "If you have any additional questions or concerns, don't hesitate to let me know. I'm here to help you every step of the way.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save LoRA adapter\n",
    "trainer.save_model()  # saves to output_dir\n",
    "\n",
    "# Quick test generation\n",
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a helpful customer support assistant.\"},\n",
    "    {\"role\":\"user\",\"content\":\"How can I track my order?\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "gen = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    streamer=streamer\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
