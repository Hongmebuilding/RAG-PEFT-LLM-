{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b657da-5307-4edd-b026-44a11d0c4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install --upgrade trl transformers datasets peft accelerate bitsandbytes huggingface_hub hf_transfer sentencepiece \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d0adb78-058c-4e49-b092-b9869aeccb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF whoami: {'type': 'user', 'id': '6900911d2cafc5f572673a1c', 'name': 'survd0404', 'fullname': 'Lee', 'isPro': False, 'avatarUrl': '/avatars/6738b5fc2f71a66ab3ca028ab9a5da26.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'HF_TOKEN', 'role': 'fineGrained', 'createdAt': '2025-11-05T14:30:18.423Z', 'fineGrained': {'canReadGatedRepos': True, 'global': ['discussion.write', 'post.write'], 'scoped': [{'entity': {'_id': '6900911d2cafc5f572673a1c', 'type': 'user', 'name': 'survd0404'}, 'permissions': ['repo.content.read', 'repo.write', 'inference.serverless.write', 'inference.endpoints.infer.write', 'inference.endpoints.write', 'user.webhooks.read', 'user.webhooks.write', 'collection.read', 'collection.write', 'discussion.write']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Hugging Face login helper ---\n",
    "try:\n",
    "    from huggingface_hub import login, whoami\n",
    "    import os\n",
    "    if os.environ.get(\"HF_TOKEN\"):\n",
    "        login(token=os.environ[\"HF_TOKEN\"])\n",
    "        try:\n",
    "            print(\"HF whoami:\", whoami())\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        print(\" Set HF_TOKEN env var or run:\")\n",
    "        print(\"   from huggingface_hub import login; login(token='hf_...')\")\n",
    "except Exception as e:\n",
    "    print(\"Hugging Face login not available:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1760d5c7-9ad4-41cf-9af3-4a3cc3b52e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== CONFIG ======\n",
    "SAMPLE_TRAIN = 2000\n",
    "SAMPLE_EVAL  = 500\n",
    "MODEL_NAME = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "OUTPUT_DIR = \"Qwen2-0.5B-DPO-qlora\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abce9540-7df5-4f14-9fcb-c7c05673888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train_prefs', 'test_prefs']\n",
      "Sample 1\n",
      "prompt  : Please provide the content structure of the following text using [Latex] data format. Additionally, please ensure that the structure includes a table comparing the results of the two groups, as well a\n",
      "chosen  : I am not able to write in LaTeX directly in this text box, but I can provide you with a template that you can use for your text. You can copy this template into a LaTeX document and modify it as neede\n",
      "rejected: According to the article titled \"Title of Article\", the following is the Latex format data structure of the content:\n",
      "\n",
      "\\begin{table}[htbp]\n",
      "\n",
      "\\centering\n",
      "\\caption{Table comparing the results of the two gr\n",
      "------------------------------\n",
      "Sample 2\n",
      "prompt  : Summorize the movie Beer from 1985\n",
      "chosen  : \"Beer,\" also known as \"The Selling of America\" or \"Beer: The Movie,\" is a 1985 comedy film directed by Patrick Kelly. The story revolves around the advertising world and the marketing of a beer produc\n",
      "rejected: \"Beer\" (also known as \"The Selling of America\") is a 1985 comedy film directed by Patrick Kelly. The movie revolves around an advertising executive named B.D. Tucker (played by Loretta Swit) who is gi\n",
      "------------------------------\n",
      "Sample 3\n",
      "prompt  : api authentication in nextjs 13\n",
      "Post: https://simple-books-api.glitch.me/api-clients/\n",
      "{\n",
      " \"clientName\": \"Postman\",\n",
      " \"clientEmail\": \"sajid@example.com\"\n",
      "}\n",
      "\n",
      "{\n",
      " \"accessToken\": \"918df582b1998acbd2cb32c66b29\n",
      "chosen  : I'm not able to directly access or interact with external APIs or services. However, I can provide some general guidance on how to authenticate with an API using Next.js.\n",
      "\n",
      "Firstly, it's important to n\n",
      "rejected: {\"result\": \"Successfully authenticated client 'Postman' with access token 918df582b1998acbd2cb32c66b2965ef19a8dc596f24592aba04bb598d8f3637\"}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# === Replace split selection with this ===\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "raw_all = load_dataset(\"SAGI-1/ultrafeedback_binarized_dpo\")\n",
    "print(\"Available splits:\", list(raw_all.keys()))  # ['train_prefs', 'test_prefs']\n",
    "\n",
    "# Hard-map the known splits\n",
    "raw = DatasetDict({\n",
    "    \"train\": raw_all[\"train_prefs\"],\n",
    "    \"validation\": raw_all[\"test_prefs\"],   # use test_prefs as eval/validation\n",
    "})\n",
    "\n",
    "# Sample down for dev speed\n",
    "train_small = raw[\"train\"].shuffle(seed=SEED).select(range(min(SAMPLE_TRAIN, len(raw[\"train\"])) ))\n",
    "eval_small  = raw[\"validation\"].shuffle(seed=SEED).select(range(min(SAMPLE_EVAL,  len(raw[\"validation\"])) ))\n",
    "\n",
    "(train_small, eval_small)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}\")\n",
    "    print(\"prompt  :\", train_small[i][\"prompt\"][:200])\n",
    "    print(\"chosen  :\", train_small[i][\"chosen\"][:200])\n",
    "    print(\"rejected:\", train_small[i][\"rejected\"][:200])\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9987f94f-c60a-4235-b2c3-7606ed4738c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5848b7ec6fb4ac5a860060f587fb281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/61966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50f09fe896b476e9951ab487dac3cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/61966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3353aa83be441ebce4fd3665d41f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/61966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13731e70072242b2856b2a88e60dac30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fbd319e7f44b80935836e8caaf4b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150bd7690b704f1cac47bf6e795e5e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 1:30:28, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.655600</td>\n",
       "      <td>0.638345</td>\n",
       "      <td>0.079143</td>\n",
       "      <td>-0.068675</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.147818</td>\n",
       "      <td>-413.460907</td>\n",
       "      <td>-325.393707</td>\n",
       "      <td>-2.775473</td>\n",
       "      <td>-2.774876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.626600</td>\n",
       "      <td>0.614039</td>\n",
       "      <td>0.109011</td>\n",
       "      <td>-0.140045</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.249056</td>\n",
       "      <td>-413.162231</td>\n",
       "      <td>-326.107452</td>\n",
       "      <td>-2.795579</td>\n",
       "      <td>-2.796163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.596600</td>\n",
       "      <td>0.607598</td>\n",
       "      <td>0.141763</td>\n",
       "      <td>-0.146722</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.288485</td>\n",
       "      <td>-412.834686</td>\n",
       "      <td>-326.174194</td>\n",
       "      <td>-2.810485</td>\n",
       "      <td>-2.811951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.613500</td>\n",
       "      <td>0.603034</td>\n",
       "      <td>0.150612</td>\n",
       "      <td>-0.163944</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.314556</td>\n",
       "      <td>-412.746185</td>\n",
       "      <td>-326.346405</td>\n",
       "      <td>-2.811163</td>\n",
       "      <td>-2.813258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.600783</td>\n",
       "      <td>0.154086</td>\n",
       "      <td>-0.171658</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.325744</td>\n",
       "      <td>-412.711456</td>\n",
       "      <td>-326.423523</td>\n",
       "      <td>-2.811383</td>\n",
       "      <td>-2.813726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "\n",
    "train_dataset = load_dataset(\"SAGI-1/ultrafeedback_binarized_dpo\", split=\"train_prefs\")\n",
    "eval_dataset  = load_dataset(\"SAGI-1/ultrafeedback_binarized_dpo\", split=\"test_prefs\")\n",
    "\n",
    "# ===== 4bit Quantization (QLoRA) =====\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    quantization_config=quant_config,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "# ===== LoRA (PEFT) =====\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],  # Qwen2 계열\n",
    ")\n",
    "\n",
    "# ===== DPO 설정 =====\n",
    "dpo_cfg = DPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    seed=SEED,\n",
    "    # 메모리 안정화 세팅\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,     # VRAM 부족 시 늘리기\n",
    "    learning_rate=5e-6,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    evaluation===_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    optim=\"paged_adamw_8bit\",          # 4bit에 적합한 옵티마이저\n",
    "    # DPO 전용/길이 제한\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,             # 너무 길면 OOM → 512/768 부터 점진 증가\n",
    "    max_length=768,                    # 응답 포함 총 길이 상한\n",
    "    max_steps=1000,                    # 개발용; 안정화되면 epochs로 전환 가능\n",
    "    remove_unused_columns=False,       # DPO에서 안전\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# ===== Trainer =====\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=dpo_cfg,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0e8dbf4-a640-4796-a7d2-1c26643d345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample idx 41905 ===\n",
      "[PROMPT] Name two African countries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298/956621076.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx = (torch.cuda.amp.autocast(dtype=AMP_DTYPE) if AMP_ENABLED else contextlib.nullcontext())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GENERATED]\n",
      " system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Name two African countries\n",
      "assistant\n",
      "Two African countries that you might be thinking of are:\n",
      "\n",
      "1. Nigeria: This country is the largest in Africa and has a rich history, culture, and language. It is also known for its rich oil reserves.\n",
      "\n",
      "2. South Sudan: Also known as the \"Sudanese Republic of Southern Sudan,\" this country gained independence from Ethiopia on December 10, 1963, and it has since become an independent nation. The country's economy is heavily ...\n",
      "\n",
      "=== Sample idx 7296 ===\n",
      "[PROMPT] In this task, you have to generate the title of the recipe given its required ingredients and directions.\n",
      "--------\n",
      "Question: ingredients:  '1/2 cup whole milk or 1/2 cup low-fat milk, cold', '1/2 cup ...\n",
      "\n",
      "[GENERATED]\n",
      " system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "In this task, you have to generate the title of the recipe given its required ingredients and directions.\n",
      "--------\n",
      "Question: ingredients:  '1/2 cup whole milk or 1/2 cup low-fat milk, cold', '1/2 cup pecans, chopped, toasted', '3 tablespoons pure maple syrup', '1 quart butter pecan ice cream, softened until just melty at the edges',<sep> directions: 'Place milk, pecans, and maple syrup in a blender and blend to break down the pecans and mix thoroughly, ab...\n"
     ]
    }
   ],
   "source": [
    "# ===== Inference: quick sanity check on a couple of train samples =====\n",
    "import random, torch\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# (추가) 모델 실제 dtype 감지 → bf16/float16 자동 선택\n",
    "def _infer_model_dtype(model):\n",
    "    try:\n",
    "        return next(model.parameters()).dtype\n",
    "    except StopIteration:\n",
    "        return torch.float16  # fallback\n",
    "\n",
    "MODEL_DTYPE = _infer_model_dtype(model)\n",
    "AMP_DTYPE = torch.bfloat16 if MODEL_DTYPE == torch.bfloat16 else torch.float16\n",
    "AMP_ENABLED = torch.cuda.is_available()\n",
    "\n",
    "# 1) 안전한 prompt 추출기 (데이터 스키마가 달라도 동작)\n",
    "def _extract_prompt(ex):\n",
    "    for k in [\"prompt\", \"instruction\", \"question\", \"query\", \"input\"]:\n",
    "        if k in ex and isinstance(ex[k], str) and ex[k].strip():\n",
    "            return ex[k]\n",
    "    if \"messages\" in ex and isinstance(ex[\"messages\"], list):\n",
    "        for msg in reversed(ex[\"messages\"]):\n",
    "            if isinstance(msg, dict) and msg.get(\"role\") == \"user\" and isinstance(msg.get(\"content\"), str):\n",
    "                return msg[\"content\"]\n",
    "    return ex.get(\"prompt\", \"\") or ex.get(\"instruction\", \"\") or \"\"\n",
    "\n",
    "# 2) chat template 있으면 사용, 없으면 간단 포맷\n",
    "def _format_for_gen(tokenizer, prompt: str) -> str:\n",
    "    tmpl = getattr(tokenizer, \"chat_template\", None)\n",
    "    if tmpl:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return f\"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
    "\n",
    "# 3) 한 샘플 생성  <-- 여기 수정!\n",
    "def generate_reply(model, tokenizer, prompt: str,\n",
    "                   max_new_tokens=128, temperature=0.7, top_p=0.9):\n",
    "    text = _format_for_gen(tokenizer, prompt)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # 모델 dtype에 맞춰 autocast (bf16이면 bf16, 아니면 fp16)\n",
    "        ctx = (torch.cuda.amp.autocast(dtype=AMP_DTYPE) if AMP_ENABLED else contextlib.nullcontext())\n",
    "        with ctx:\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# 4) 학습셋에서 1~2개 뽑아 스모크 테스트\n",
    "random.seed(42)\n",
    "num_samples = min(2, len(train_dataset))\n",
    "indices = random.sample(range(len(train_dataset)), k=num_samples)\n",
    "\n",
    "for idx in indices:\n",
    "    ex = train_dataset[int(idx)]\n",
    "    prompt = _extract_prompt(ex) or \"Briefly explain what DPO is.\"\n",
    "    print(f\"\\n=== Sample idx {idx} ===\")\n",
    "    print(\"[PROMPT]\", prompt[:200] + (\"...\" if len(prompt) > 200 else \"\"))\n",
    "    output = generate_reply(model, tokenizer, prompt,\n",
    "                            max_new_tokens=128, temperature=0.7)\n",
    "    print(\"\\n[GENERATED]\\n\", output[:500] + (\"...\" if len(output) > 500 else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7a73c-4199-4d37-8a0e-f0b845327c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51277ac4-096c-4fb4-aec9-22c578d9b478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
