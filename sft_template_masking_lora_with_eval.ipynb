{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ad1f38",
   "metadata": {},
   "source": [
    "\n",
    "# SFT with Template Masking (LoRA) — Argilla/Bitext + Dolly (7B)\n",
    "\n",
    "This notebook fine-tunes a chat model (e.g., `meta-llama/Llama-2-7b-hf`) using **template masking** (no collator), so only the **assistant** parts are trained.  \n",
    "Key points:\n",
    "\n",
    "- **No DataCollator** required. We use the tokenizer's **chat template** with `{% generation %}...{% endgeneration %}` to mask assistant turns.\n",
    "- `packing=True` works (higher throughput) since masking is handled by the chat template.\n",
    "- `tokenizer.truncation_side=\"left\"` to preserve the assistant turn at the end of long sequences.\n",
    "- Supports Argilla/Bitext customer-support datasets + Dolly-15k subset.\n",
    "- Uses **LoRA** (PEFT) to reduce VRAM usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9275a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m136.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m166.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m154.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m177.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m190.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, safetensors, regex, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, bitsandbytes, accelerate, peft, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [datasets]/25\u001b[0m [datasets]e]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 bitsandbytes-0.48.2 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 hf-xet-1.2.0 huggingface-hub-0.36.0 multidict-6.7.0 multiprocess-0.70.18 pandas-2.3.3 peft-0.17.1 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 regex-2025.11.3 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%capture\n",
    "# # (Optional) Install versions known to work well together\n",
    "!pip install -U transformers accelerate datasets peft bitsandbytes hf_transfer\n",
    "!pip install -U trl>=0.25.0\n",
    "# # (Optional) flash-attn (environment dependent)\n",
    "# !pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711dad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, Any, List\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8177da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF whoami: {'type': 'user', 'id': '6900911d2cafc5f572673a1c', 'name': 'survd0404', 'fullname': 'Lee', 'isPro': False, 'avatarUrl': '/avatars/6738b5fc2f71a66ab3ca028ab9a5da26.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'HF_TOKEN', 'role': 'fineGrained', 'createdAt': '2025-11-05T14:30:18.423Z', 'fineGrained': {'canReadGatedRepos': True, 'global': ['discussion.write', 'post.write'], 'scoped': [{'entity': {'_id': '6900911d2cafc5f572673a1c', 'type': 'user', 'name': 'survd0404'}, 'permissions': ['repo.content.read', 'repo.write', 'inference.serverless.write', 'inference.endpoints.infer.write', 'inference.endpoints.write', 'user.webhooks.read', 'user.webhooks.write', 'collection.read', 'collection.write', 'discussion.write']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Hugging Face login helper ---\n",
    "try:\n",
    "    from huggingface_hub import login, whoami\n",
    "    import os\n",
    "    if os.environ.get(\"HF_TOKEN\"):\n",
    "        login(token=os.environ[\"HF_TOKEN\"])\n",
    "        try:\n",
    "            print(\"HF whoami:\", whoami())\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        print(\" Set HF_TOKEN env var or run:\")\n",
    "        print(\"   from huggingface_hub import login; login(token='hf_...')\")\n",
    "except Exception as e:\n",
    "    print(\"Hugging Face login not available:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "596eecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387991589fc74befabc921baaae58676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c5876517764e6d9f191308ab4c3872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480963099ce74f2c926a6250f69d3cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62ddea38ec74c2cb438e2e63fe3ed2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae18884882994929b5eeba45eeaf9671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6295e95426464938bc3d68968e095522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8449292db475492891bbe0a4abe1f972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab917bd5c55419abb5495009da00cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29ed24a887340128b6f84c42212e42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8d3b34e58a4ff7ba7e92586c3b4d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose your base model (7B chat model recommended)\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"meta-llama/Llama-2-7b-hf\")\n",
    "# You must have access to the model on HF. If needed: `huggingface-cli login`\n",
    "\n",
    "print(\"Loading tokenizer:\", MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# Truncation/padding so that assistant turn (at the end) is kept\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.padding_side    = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model:\", MODEL_NAME)\n",
    "# Use bf16 if on multi-GPU auto mapping; fp16 if single 24GB GPU for lower VRAM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",     # change to torch.bfloat16 / torch.float16 explicitly if desired\n",
    "    device_map=\"auto\"       # let HF place layers automatically if multiple GPUs\n",
    ")\n",
    "# Disable cache during training\n",
    "if hasattr(model, \"config\"):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "print(\"Model & tokenizer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad23501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat template with assistant masking is set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a chat template that masks assistant turns only.\n",
    "# With this, SFTTrainer will compute loss only on `{% generation %}` blocks.\n",
    "tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "<|{{ message['role'] }}|>\n",
    "{% if message['role'] == 'assistant' -%}\n",
    "{% generation %}\n",
    "{{ message['content'] }}\n",
    "{% endgeneration %}\n",
    "{%- else -%}\n",
    "{{ message['content'] }}\n",
    "{%- endif %}\n",
    "\n",
    "{% endfor %}\"\"\"\n",
    "\n",
    "print(\"Chat template with assistant masking is set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4450b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Datasets to pull\n",
    "ARGILLA_BITEXT = [\n",
    "    (\"argilla/customer_assistant\", {}),\n",
    "    (\"argilla/synthetic-sft-customer-support-single-turn\", {}),\n",
    "    (\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\", {}),\n",
    "]\n",
    "DOLLY_SLICE = \"train[:1000]\"  # Adjust as needed\n",
    "\n",
    "def extract_user_assistant(record: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Map heterogeneous keys to a unified (system, user, assistant) triple.\"\"\"\n",
    "    keys = set(record.keys())\n",
    "\n",
    "    user_candidates = [\n",
    "        \"user\",\"question\",\"prompt\",\"instruction\",\"input\",\"request\",\"query\",\n",
    "        \"messages_user\",\"customer\",\"customer_message\"\n",
    "    ]\n",
    "    assistant_candidates = [\n",
    "        \"assistant\",\"answer\",\"response\",\"output\",\"messages_assistant\",\n",
    "        \"agent\",\"agent_response\"\n",
    "    ]\n",
    "    system_candidates = [\"system\",\"context\",\"role\",\"scenario\"]\n",
    "\n",
    "    def pick(cands):\n",
    "        # 1) flat keys\n",
    "        for c in cands:\n",
    "            if c in record and isinstance(record[c], str) and len(record[c].strip())>0:\n",
    "                return record[c]\n",
    "        # 2) nested messages\n",
    "        for k in keys:\n",
    "            v = record[k]\n",
    "            if isinstance(v, list) and len(v)>0 and isinstance(v[0], dict) and \"role\" in v[0] and \"content\" in v[0]:\n",
    "                u, a, s = None, None, None\n",
    "                for m in v:\n",
    "                    role = (m.get(\"role\",\"\") or \"\").lower()\n",
    "                    content = m.get(\"content\",\"\")\n",
    "                    if role == \"system\" and not s:\n",
    "                        s = content\n",
    "                    if role in (\"user\",\"customer\") and not u:\n",
    "                        u = content\n",
    "                    if role in (\"assistant\",\"agent\") and not a:\n",
    "                        a = content\n",
    "                if cands is user_candidates and u:\n",
    "                    return u\n",
    "                if cands is assistant_candidates and a:\n",
    "                    return a\n",
    "                if cands is system_candidates and s:\n",
    "                    return s\n",
    "        return None\n",
    "\n",
    "    user = pick(user_candidates)\n",
    "    assistant = pick(assistant_candidates)\n",
    "    system = pick(system_candidates) or \"You are a helpful, step-by-step customer support assistant.\"\n",
    "\n",
    "    # fallback: synthesize from any two text fields\n",
    "    if not user or not assistant:\n",
    "        textish = [k for k in keys if isinstance(record.get(k), str) and len(record[k].strip())>0]\n",
    "        if len(textish)>=2 and not user:\n",
    "            user = record[textish[0]]\n",
    "        if len(textish)>=2 and not assistant:\n",
    "            assistant = record[textish[1]]\n",
    "\n",
    "    if not user or not assistant:\n",
    "        raise ValueError(\"Could not map record to user/assistant\")\n",
    "\n",
    "    return {\"system\": system.strip(), \"user\": user.strip(), \"assistant\": assistant.strip()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0389b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_to_messages(hf_id: str, hf_config: Dict[str, Any]):\n",
    "    ds = load_dataset(hf_id, **hf_config)\n",
    "    split_names = list(ds.keys()) if isinstance(ds, DatasetDict) else [\"train\"]\n",
    "    out_splits = []\n",
    "\n",
    "    for split in split_names:\n",
    "        d = ds[split] if isinstance(ds, DatasetDict) else ds\n",
    "        def _map(rec):\n",
    "            try:\n",
    "                ex = extract_user_assistant(rec)\n",
    "                msgs = []\n",
    "                if ex.get(\"system\"):\n",
    "                    msgs.append({\"role\":\"system\",\"content\":ex[\"system\"]})\n",
    "                msgs.append({\"role\":\"user\",\"content\":ex[\"user\"]})\n",
    "                msgs.append({\"role\":\"assistant\",\"content\":ex[\"assistant\"]})\n",
    "                return {\"messages\": msgs}\n",
    "            except Exception:\n",
    "                return {\"messages\": None}\n",
    "\n",
    "        d2 = d.map(_map, remove_columns=[c for c in d.column_names if c!=\"messages\"])\n",
    "        d2 = d2.filter(lambda r: r[\"messages\"] is not None)\n",
    "        out_splits.append(d2)\n",
    "\n",
    "    merged = concatenate_datasets(out_splits) if len(out_splits)>1 else out_splits[0]\n",
    "    return merged\n",
    "\n",
    "def build_argilla_bitext_messages():\n",
    "    all_ds = []\n",
    "    for name, cfg in ARGILLA_BITEXT:\n",
    "        print(\"Loading:\", name, cfg)\n",
    "        try:\n",
    "            all_ds.append(preprocess_to_messages(name, cfg))\n",
    "        except Exception as e:\n",
    "            print(\"!! Skipped\", name, \"due to\", e)\n",
    "    if not all_ds:\n",
    "        raise RuntimeError(\"No argilla/bitext datasets loaded — check access/names.\")\n",
    "    merged = concatenate_datasets(all_ds)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3da48967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_dolly_messages(slice_spec=DOLLY_SLICE):\n",
    "    ds = load_dataset(\"databricks/databricks-dolly-15k\", split=slice_spec)\n",
    "    def _fmt(sample):\n",
    "        instruction = sample.get(\"instruction\",\"\") or \"\"\n",
    "        context = sample.get(\"context\",\"\") or \"\"\n",
    "        response = sample.get(\"response\",\"\") or \"\"\n",
    "        user = f\"{instruction.strip()} {context.strip()}\".strip()\n",
    "        assistant = response.strip()\n",
    "        msgs = []\n",
    "        # Dolly usually has no system by default\n",
    "        if user:\n",
    "            msgs.append({\"role\":\"user\",\"content\":user})\n",
    "        if assistant:\n",
    "            msgs.append({\"role\":\"assistant\",\"content\":assistant})\n",
    "        return {\"messages\": msgs if msgs else None}\n",
    "\n",
    "    ds = ds.map(_fmt, remove_columns=[c for c in ds.column_names if c!=\"messages\"])\n",
    "    ds = ds.filter(lambda r: r[\"messages\"] is not None)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3366ce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: argilla/customer_assistant {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d7105353344043a5fe35c1419c7c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c627f2f9d894bf2ab2e1b9dd0576804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-6853b30b1d9b88(…):   0%|          | 0.00/76.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc46a6dd7d2c4ed7b25b47251cb225ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5eec6e6ac41407db25ba35be6b499aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4226944f55ea46dba3aa0310e2ec5cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: argilla/synthetic-sft-customer-support-single-turn {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc964fb016324aaf8cc7e0ac92f3d9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54381c911eda4531a4f87df8c236384d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/133k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2027d821481b41acb3fa5eaf98953127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15a0099404242cca1d00043c26fb71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2c1543a52541058b3b1a3e78c5fe66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: bitext/Bitext-customer-support-llm-chatbot-training-dataset {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe5a9623beb423b8a76920ffcd2bd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324382e509204fd6a1f1b5cd444ea04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Bitext_Sample_Customer_Support_Training_(…):   0%|          | 0.00/19.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae0c80afde045a7be781db5af0c97c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/26872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6e8b131d984c70beb670e8367f5fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ed2a3b15c04e3e8e4c4cb29a3220a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0a7c47108b4ca189f3a2a67955747f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a572511ab4d462b95ca158bf43f0e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7a20c9a91149b0b949615a1360ed4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d9d60d112847338570886ac69860b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35fb46cedc24b9cbc41d5181ce85003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged size (raw): 28168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb6c66eeaea46a1a3a0093f04b9f062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/28168 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c9f8c80e9a44b4af4ce2721f33aa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/28168 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After basic filtering: 28168\n",
      "---- <|system|>\n",
      "You are a helpful, step-by-step customer support assistant.\n",
      "<|user|>\n",
      "I want help seeing in which cases can I ask for a refund\n",
      "<|assistant|>\n",
      "Of course, I'm here to provide you with a detailed understanding of the cases in which you can request a refund. Here are some common scenarios:\n",
      "\n",
      "1. **Product/Service Defect:** If the product or service you purchased is defective, damaged, or doesn'\n",
      "---- <|system|>\n",
      "You are a helpful, step-by-step customer support assistant.\n",
      "<|user|>\n",
      "i have a problem setting a delivery address up\n",
      "<|assistant|>\n",
      "I apologize for the difficulties you're experiencing while setting up your delivery address. I'm here to assist you in resolving this issue.\n",
      "\n",
      "Setting up a delivery address is an essential part of the process, and I completely understand the importance of gett\n"
     ]
    }
   ],
   "source": [
    "\n",
    "argilla_bitext = build_argilla_bitext_messages()\n",
    "dolly_subset   = build_dolly_messages(DOLLY_SLICE)\n",
    "\n",
    "dataset = concatenate_datasets([argilla_bitext, dolly_subset]).shuffle(seed=SEED)\n",
    "print(\"Merged size (raw):\", len(dataset))\n",
    "\n",
    "def has_assistant(ex):\n",
    "    m = ex.get(\"messages\", [])\n",
    "    roles = [x.get(\"role\") for x in m]\n",
    "    return isinstance(m, list) and (\"assistant\" in roles)\n",
    "\n",
    "def min_tokens_chat(ex, min_tokens=8):\n",
    "    s = tokenizer.apply_chat_template(ex[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "    ids = tokenizer(s, add_special_tokens=False).input_ids\n",
    "    return ids is not None and len(ids) >= min_tokens\n",
    "\n",
    "dataset = dataset.filter(has_assistant)\n",
    "dataset = dataset.filter(min_tokens_chat)\n",
    "print(\"After basic filtering:\", len(dataset))\n",
    "\n",
    "# Quick peek\n",
    "for i in range(2):\n",
    "    s = tokenizer.apply_chat_template(dataset[i][\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "    print(\"----\", s[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6668c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 28068 eval: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = len(dataset)\n",
    "k = min(100, max(1, n // 50))  # about 2% or cap at 100\n",
    "split = dataset.train_test_split(test_size=k, seed=SEED, shuffle=True)\n",
    "train_dataset, eval_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(\"train:\", len(train_dataset), \"eval:\", len(eval_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12049e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA config ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],  # add k_proj/o_proj if needed\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "print(\"LoRA config ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c44e410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTConfig(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "activation_offloading=False,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "assistant_only_loss=False,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=True,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "chat_template_path=None,\n",
      "completion_only_loss=None,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset_kwargs=None,\n",
      "dataset_num_proc=None,\n",
      "dataset_text_field=text,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eos_token=<EOS_TOKEN>,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_packing=None,\n",
      "eval_steps=100,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=no,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./sft_custom_results/runs/Nov13_08-24-06_f1f772b2efa9,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=20,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "loss_type=nll,\n",
      "lr_scheduler_kwargs=None,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_length=512,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "model_init_kwargs=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./sft_custom_results,\n",
      "overwrite_output_dir=False,\n",
      "packing=True,\n",
      "packing_strategy=bfd,\n",
      "pad_to_multiple_of=None,\n",
      "pad_token=<PAD_TOKEN>,\n",
      "padding_free=False,\n",
      "parallelism_config=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "project=huggingface,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=200,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trackio_space_id=trackio,\n",
      "use_cpu=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=\"./sft_custom_results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=True,                 # On multi-GPU auto mapping, bf16=True can be safer\n",
    "    packing=True,              # ✅ works with template masking\n",
    "    max_length=512,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,   # big VRAM saver\n",
    ")\n",
    "print(sft_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fda7962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32f44c1fb4e456da77dd9d07e998127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/28068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4489ee8f9e544124ba757c971843ec83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/28068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61c5e672df241ccb957a31812fc0f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0278dc02204a18940a35fc12607304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready. Rendering one sample after template:\n",
      "<|system|>\n",
      "You are a helpful, step-by-step customer support assistant.\n",
      "<|user|>\n",
      "i have to see the status of purchase {{Order Number}} how do i do it\n",
      "<|assistant|>\n",
      "Glad you contacted us! I'm clearly cognizant that you would like to check the status of your purchase with order number {{Order Number}}. To do so, you can navigate to the 'Order Details' section on our website. This section will provide you with all the information regarding your purchase, including its current status. If you have any further questions or need additional assistance, please don't hesitate to ask. I'm here to help!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_args,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=None,\n",
    ")\n",
    "print(\"Trainer ready. Rendering one sample after template:\")\n",
    "print(tokenizer.apply_chat_template(train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False)[:600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17fcab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 30:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.414800</td>\n",
       "      <td>1.361153</td>\n",
       "      <td>1.464559</td>\n",
       "      <td>799604.000000</td>\n",
       "      <td>0.662065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>1.030468</td>\n",
       "      <td>1.017812</td>\n",
       "      <td>1599955.000000</td>\n",
       "      <td>0.729319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.914500</td>\n",
       "      <td>0.969202</td>\n",
       "      <td>0.962321</td>\n",
       "      <td>2398635.000000</td>\n",
       "      <td>0.740033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.904600</td>\n",
       "      <td>0.930884</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>3197100.000000</td>\n",
       "      <td>0.747152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.870700</td>\n",
       "      <td>0.907071</td>\n",
       "      <td>0.904815</td>\n",
       "      <td>3997432.000000</td>\n",
       "      <td>0.751359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>0.895056</td>\n",
       "      <td>0.897542</td>\n",
       "      <td>4798588.000000</td>\n",
       "      <td>0.754803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "TrainOutput(global_step=670, training_loss=1.0405721821002105, metrics={'train_runtime': 1830.4509, 'train_samples_per_second': 5.855, 'train_steps_per_second': 0.366, 'total_flos': 2.1264560405694874e+17, 'train_loss': 1.0405721821002105, 'epoch': 1.0})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 🚀 Train\n",
    "train_result = trainer.train()\n",
    "print(\"Training complete.\")\n",
    "print(train_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "162d7c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|assistant|>\n",
      "To track your order, please provide me with the following information:\n",
      "\n",
      "1. Your order number: {{Order Number}}\n",
      "2. The shipping or tracking number associated with your order: {{Shipping or Tracking Number}}\n",
      "\n",
      "Once I have these details, I will be able to provide you with the most up-to-date information on the status of your order. If you have any further questions or concerns, please feel free to reach out to us. We're here to help!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save LoRA adapter\n",
    "trainer.save_model()  # saves to output_dir\n",
    "\n",
    "# Quick test generation\n",
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a helpful customer support assistant.\"},\n",
    "    {\"role\":\"user\",\"content\":\"How can I track my order?\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "gen = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    streamer=streamer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c4d35",
   "metadata": {},
   "source": [
    "\n",
    "## 🔍 모델 평가 (G-EVAL + RAG + 벤치마크 훅)\n",
    "\n",
    "이 섹션에서는 다음을 수행할 수 있도록 평가 코드를 추가했습니다.\n",
    "\n",
    "1. **G-EVAL 스타일 항목별 평가**\n",
    "   - 평가 모델: **Qwen2.5** (예: `Qwen2.5-7B-Instruct`와 같이 실제 HF ID로 교체)\n",
    "   - 항목별 평가 프롬프트(예: helpfulness, correctness, style 등)를 사용해 **1~10 점수** 산출\n",
    "   - 학습 전 **베이스라인 모델**, LoRA 학습 후 **업데이트 모델** 각각에 대해 score 계산\n",
    "\n",
    "2. **(선택) RAG 평가**\n",
    "   - 주어진 정답 context 및 검색된 context 목록으로부터 **precision@k, recall@k** 계산 유틸 함수 제공\n",
    "\n",
    "3. **기타 벤치마크 훅**\n",
    "   - `mmlu`, `mt-bench` 등 외부 라이브러리를 이용한 평가를 쉽게 붙일 수 있도록 스켈레톤 코드 제공\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bf9e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluator model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f567fb8401c4d4b9476ece78c52f537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b2a524c0224c099969dec3ebae3036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb349c792604f948bbe9a627c5a462f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81673ef838014f659e9f65cd12cdb40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2faa1358c1a044f491a00bf64481dfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97922b9e7da44160a5e559425b2e65db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4512bedf2ddb41b4a86d9d0272338d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e0c651435f41d783f6fcf96740ab07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4ca0b32f54494ab717a06094dd0718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7297a1b69cd04b2186448ce72a66d5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af891633c9e4515a5eb6d859193c97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4809093725b24f3ab9e5e6c40ccf3783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline model for evaluation: meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7632d4cff2c9436db0dda180870ac38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing G-EVAL scores for baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] average scores: {'helpfulness': 2.125, 'correctness': 2.375, 'style': 3.125}\n",
      "Computing G-EVAL scores for updated (LoRA) model...\n",
      "[Updated] average scores: {'helpfulness': 6.625, 'correctness': 6.1875, 'style': 7.5625}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import torch\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# ==========================\n",
    "# G-EVAL 설정\n",
    "# ==========================\n",
    "\n",
    "# ✏️ 실제 사용하는 Qwen3-8B 평가 모델 ID로 바꿔주세요.\n",
    "# 예시: \"Qwen2.5-Instruct\" (실제 HF Hub 모델명 확인 후 교체)\n",
    "QWEN_EVAL_MODEL_NAME = os.environ.get(\"QWEN_EVAL_MODEL_NAME\", \"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "print(\"Loading evaluator model:\", QWEN_EVAL_MODEL_NAME)\n",
    "from transformers import AutoTokenizer as EvalAutoTokenizer, AutoModelForCausalLM as EvalAutoModelForCausalLM\n",
    "\n",
    "evaluator_tokenizer = EvalAutoTokenizer.from_pretrained(QWEN_EVAL_MODEL_NAME, use_fast=True)\n",
    "evaluator_model = EvalAutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_EVAL_MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "evaluator_model.eval()\n",
    "\n",
    "# 평가 항목별 프롬프트 템플릿 (원래 제공받은 프롬프트를 여기에 그대로 넣으면 됨)\n",
    "# 각 value는 전체 시스템 prompt가 아니라, \"평가 기준 설명 + 출력 형식\"을 정의하는 템플릿입니다.\n",
    "# {{user}}, {{reference}}, {{candidate}} 자리에 실제 텍스트를 넣습니다.\n",
    "EVAL_CRITERIA_PROMPTS: Dict[str, str] = {\n",
    "    \"helpfulness\": (\n",
    "        \"You are an expert judge for customer support responses.\\n\"\n",
    "        \"Evaluate how helpful the assistant's answer is to the user's question.\\n\"\n",
    "        \"User question:\\n{{user}}\\n\\n\"\n",
    "        \"Reference (ideal) answer:\\n{{reference}}\\n\\n\"\n",
    "        \"Candidate answer:\\n{{candidate}}\\n\\n\"\n",
    "        \"Give a score from 1 (very bad) to 10 (excellent).\\n\"\n",
    "        \"Output ONLY in the format: Score: <number>.\"\n",
    "    ),\n",
    "    \"correctness\": (\n",
    "        \"You are an expert judge for factual correctness.\\n\"\n",
    "        \"Evaluate how factually correct the candidate answer is compared to the reference.\\n\"\n",
    "        \"User question:\\n{{user}}\\n\\n\"\n",
    "        \"Reference (ground-truth) answer:\\n{{reference}}\\n\\n\"\n",
    "        \"Candidate answer:\\n{{candidate}}\\n\\n\"\n",
    "        \"Give a score from 1 (very incorrect) to 10 (fully correct).\\n\"\n",
    "        \"Output ONLY in the format: Score: <number>.\"\n",
    "    ),\n",
    "    \"style\": (\n",
    "        \"You are an expert judge for style and politeness.\\n\"\n",
    "        \"Evaluate if the candidate answer is polite, clear, and appropriate for customer support.\\n\"\n",
    "        \"User question:\\n{{user}}\\n\\n\"\n",
    "        \"Candidate answer:\\n{{candidate}}\\n\\n\"\n",
    "        \"Give a score from 1 (very poor) to 10 (excellent).\\n\"\n",
    "        \"Output ONLY in the format: Score: <number>.\"\n",
    "    ),\n",
    "    # 필요 시 다른 항목도 동일한 형식으로 추가\n",
    "}\n",
    "\n",
    "def build_g_eval_prompt(\n",
    "    criterion: str,\n",
    "    user: str,\n",
    "    reference: str,\n",
    "    candidate: str,\n",
    ") -> str:\n",
    "    \"\"\"criterion에 해당하는 템플릿에 user/reference/candidate를 채워 넣어 평가 prompt 생성\"\"\"\n",
    "    template = EVAL_CRITERIA_PROMPTS[criterion]\n",
    "    prompt = (\n",
    "        template\n",
    "        .replace(\"{{user}}\", user or \"\")\n",
    "        .replace(\"{{reference}}\", reference or \"\")\n",
    "        .replace(\"{{candidate}}\", candidate or \"\")\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def parse_score_from_text(text: str) -> float:\n",
    "    \"\"\"LLM 출력에서 'Score: X' 형태 또는 숫자만 포함된 경우를 robust하게 파싱.\"\"\"\n",
    "    # 우선 Score: X 패턴\n",
    "    m = re.search(r\"Score\\s*[:=]\\s*([0-9]+(?:\\.[0-9]+)?)\", text)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1))\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 숫자 하나만 있는 경우 등 fallback\n",
    "    m2 = re.search(r\"([0-9]+(?:\\.[0-9]+)?)\", text)\n",
    "    if m2:\n",
    "        try:\n",
    "            return float(m2.group(1))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return float(\"nan\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_g_eval_single(\n",
    "    user: str,\n",
    "    reference: str,\n",
    "    candidate: str,\n",
    "    criterion: str,\n",
    "    max_new_tokens: int = 64,\n",
    ") -> float:\n",
    "    \"\"\"단일 샘플에 대해 Qwen3-8B로 G-EVAL 점수 계산.\"\"\"\n",
    "    prompt = build_g_eval_prompt(criterion, user, reference, candidate)\n",
    "    inputs = evaluator_tokenizer(prompt, return_tensors=\"pt\").to(evaluator_model.device)\n",
    "    outputs = evaluator_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    out_text = evaluator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    score = parse_score_from_text(out_text)\n",
    "    return score\n",
    "\n",
    "def extract_eval_triplet_from_messages(example: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"dataset의 messages에서 (system, user, reference_answer)를 뽑아냄.\"\"\"\n",
    "    messages = example.get(\"messages\", [])\n",
    "    system = None\n",
    "    last_user = None\n",
    "    last_assistant = None\n",
    "    for m in messages:\n",
    "        role = m.get(\"role\")\n",
    "        content = m.get(\"content\", \"\")\n",
    "        if role == \"system\" and system is None:\n",
    "            system = content\n",
    "        elif role == \"user\":\n",
    "            last_user = content\n",
    "        elif role == \"assistant\":\n",
    "            last_assistant = content\n",
    "    if system is None:\n",
    "        system = \"You are a helpful, step-by-step customer support assistant.\"\n",
    "    return {\n",
    "        \"system\": system,\n",
    "        \"user\": last_user or \"\",\n",
    "        \"reference\": last_assistant or \"\",\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_answer_from_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    system: str,\n",
    "    user: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    **gen_kwargs,\n",
    ") -> str:\n",
    "    \"\"\"SFT에 사용한 chat_template 기반으로 답변 생성.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    default_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    default_kwargs.update(gen_kwargs)\n",
    "    outputs = model.generate(**inputs, **default_kwargs)\n",
    "    text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================\n",
    "# 베이스라인 / 업데이트 모델 로딩\n",
    "# ==========================\n",
    "\n",
    "# 🔹 베이스라인: LoRA 적용 전 원본 모델\n",
    "print(\"Loading baseline model for evaluation:\", MODEL_NAME)\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "baseline_model.eval()\n",
    "\n",
    "# 🔹 업데이트 모델: 현재 노트북에서 LoRA로 fine-tune된 모델 (trainer 내부 모델 사용)\n",
    "#   trainer.model이 LoRA 적용 모델이므로 그대로 사용\n",
    "updated_model = trainer.model\n",
    "updated_model.eval()\n",
    "\n",
    "# ==========================\n",
    "# G-EVAL 루프: eval_dataset 일부에 대해 점수 계산\n",
    "# ==========================\n",
    "\n",
    "def compute_geval_scores_for_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_data,\n",
    "    criteria: List[str],\n",
    "    num_samples: int = 32,\n",
    "    seed: int = SEED,\n",
    "):\n",
    "    \"\"\"eval_data에서 일부 샘플을 뽑아 G-EVAL 점수 계산 후 항목별 평균 반환.\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "\n",
    "    n = len(eval_data)\n",
    "    idxs = list(range(n))\n",
    "    random.shuffle(idxs)\n",
    "    idxs = idxs[: min(num_samples, n)]\n",
    "\n",
    "    scores = {c: [] for c in criteria}\n",
    "\n",
    "    for idx in idxs:\n",
    "        ex = eval_data[idx]\n",
    "        triplet = extract_eval_triplet_from_messages(ex)\n",
    "        system = triplet[\"system\"]\n",
    "        user = triplet[\"user\"]\n",
    "        reference = triplet[\"reference\"]\n",
    "\n",
    "        candidate = generate_answer_from_model(model, tokenizer, system, user)\n",
    "\n",
    "        for c in criteria:\n",
    "            s = run_g_eval_single(\n",
    "                user=user,\n",
    "                reference=reference,\n",
    "                candidate=candidate,\n",
    "                criterion=c,\n",
    "            )\n",
    "            scores[c].append(s)\n",
    "    # 평균 계산\n",
    "    avg_scores = {c: (sum(v)/len(v) if v else float(\"nan\")) for c, v in scores.items()}\n",
    "    return avg_scores, scores\n",
    "\n",
    "CRITERIA = list(EVAL_CRITERIA_PROMPTS.keys())\n",
    "\n",
    "print(\"Computing G-EVAL scores for baseline model...\")\n",
    "baseline_avg, baseline_all = compute_geval_scores_for_model(\n",
    "    baseline_model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    criteria=CRITERIA,\n",
    "    num_samples=16,   # 필요시 조절\n",
    ")\n",
    "print(\"[Baseline] average scores:\", baseline_avg)\n",
    "\n",
    "print(\"Computing G-EVAL scores for updated (LoRA) model...\")\n",
    "updated_avg, updated_all = compute_geval_scores_for_model(\n",
    "    updated_model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    criteria=CRITERIA,\n",
    "    num_samples=16,\n",
    ")\n",
    "print(\"[Updated] average scores:\", updated_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1e269-04d9-43bf-b11d-7fdf1a92566e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
